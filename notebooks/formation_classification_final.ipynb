{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导包\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import matplotlib\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import preprocessing\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version is: 1.1.5\n",
      "numpy version is: 1.19.5\n",
      "matplotlib version is: 2.0.2\n",
      "sklearn version is: 0.24.2\n",
      "xgboost version is: 1.5.2\n"
     ]
    }
   ],
   "source": [
    "#printing out versions of all packages and libraries and used\n",
    "\n",
    "print(f'pandas version is: {pd.__version__}')\n",
    "print(f'numpy version is: {np.__version__}')\n",
    "print(f'matplotlib version is: {matplotlib.__version__}')\n",
    "print(f'sklearn version is: {sklearn.__version__}')\n",
    "print(f'xgboost version is: {xgb.__version__}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['Formation', 'Well Name', 'Depth', 'GR', 'ILD_log10', 'DeltaPHI',\n       'PHIND', 'PE', 'NM_M', 'RELPOS', 'Facies'],\n      dtype='object')"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = pd.read_csv(r'content/numpy-/美国油田/train_data_补.csv')\n",
    "# data.head()\n",
    "# np.count_nonzero(data.Formation.unique()) # 14 中Formation\n",
    "# data.Formation.unique()\n",
    "# len(data['Well Name'].unique())\n",
    "# data.columns\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# data1 = pd.read_csv(r'D:\\workspace\\pycharm\\welllogAI\\data\\处理后的FORCE数据集\\force-train.csv')\n",
    "data1 = pd.read_csv(\"../data/ciflog_scalared_fetures_med9.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['Depth', 'GR', 'INPEFA', 'AC', 'SP', 'GR_MED_3', 'GR_MED_5', 'GR_MED_7',\n       'GR_MED_9', 'GR_MED_11', 'GR_MED_13', 'GR_MED_15', 'GR_MED_17',\n       'GR_MED_19'],\n      dtype='object')"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea = data1.columns.drop(['Well Name', 'Formation'])\n",
    "fea"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------处理后的 FORCE 数据集---------------------------\n",
      "          Depth        GR    INPEFA        AC        SP  Well Name  Formation  \\\n",
      "33302 -0.648533 -0.196459  1.947425  0.004728 -0.395807          6          2   \n",
      "28818  0.124633 -0.806328 -0.960790  0.005003 -0.252663          5          0   \n",
      "34541 -0.298532  0.829419 -0.199405  0.004714  0.128416          6          3   \n",
      "3931   0.980568 -0.340667  0.429508  0.004688 -0.402720          0          5   \n",
      "6717   0.215877  0.110918  1.078388  0.004681 -0.192663          1          2   \n",
      "\n",
      "       GR_MED_3  GR_MED_5  GR_MED_7  GR_MED_9  GR_MED_11  GR_MED_13  \\\n",
      "33302 -0.196459 -0.192380 -0.192380 -0.196459  -0.196459  -0.196459   \n",
      "28818 -0.806328 -0.806328 -0.792173 -0.785805  -0.759542  -0.755392   \n",
      "34541  0.829419  0.848144  0.848144  0.848144   0.829419   0.780366   \n",
      "3931  -0.386067 -0.402593 -0.412892 -0.421130  -0.423249  -0.423249   \n",
      "6717   0.110918  0.120674  0.120674  0.120674   0.120674   0.120674   \n",
      "\n",
      "       GR_MED_15  GR_MED_17  GR_MED_19  \n",
      "33302  -0.196459  -0.196459  -0.200589  \n",
      "28818  -0.755392  -0.755392  -0.755392  \n",
      "34541   0.762590   0.755789   0.709151  \n",
      "3931   -0.421130  -0.412892  -0.404702  \n",
      "6717    0.120674   0.114076   0.110918  \n",
      "特征名称：Index(['Depth', 'GR', 'INPEFA', 'AC', 'SP', 'Well Name', 'Formation',\n",
      "       'GR_MED_3', 'GR_MED_5', 'GR_MED_7', 'GR_MED_9', 'GR_MED_11',\n",
      "       'GR_MED_13', 'GR_MED_15', 'GR_MED_17', 'GR_MED_19'],\n",
      "      dtype='object')\n",
      "行列:(92284, 16)\n",
      "共有8种地层,所有地层名称：[0 1 2 3 4 5 6 7]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'WELL_encoded'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\welllogAI\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001B[0m in \u001B[0;36mget_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   2897\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2898\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2899\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'WELL_encoded'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-4-45476aafa06d>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'行列:{data1.shape}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'共有{len(data1[\"Formation\"].unique())}种地层,所有地层名称：{data1[\"Formation\"].unique()}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'共有{len(data1[\"WELL_encoded\"].unique())}口井,所有井名称：{data1[\"WELL_encoded\"].unique()}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'共有{len(data1[\"LITHOLOGY\"].unique())}种岩性,所有岩性名称：{data1[\"LITHOLOGY\"].unique()}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'共有{len(data1[\"GROUP_encoded\"].unique())}种group,所有group名称：{data1[\"GROUP_encoded\"].unique()}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\welllogAI\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   2904\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnlevels\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2905\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getitem_multilevel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2906\u001B[1;33m             \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2907\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mis_integer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindexer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2908\u001B[0m                 \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mindexer\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\welllogAI\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001B[0m in \u001B[0;36mget_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   2898\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2899\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2900\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2901\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2902\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mtolerance\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'WELL_encoded'"
     ]
    }
   ],
   "source": [
    "print('----------------------处理后的 FORCE 数据集---------------------------')\n",
    "print(data1.sample(5))\n",
    "print(f'特征名称：{data1.columns}')\n",
    "print(f'行列:{data1.shape}')\n",
    "print(f'共有{len(data1[\"Formation\"].unique())}种地层,所有地层名称：{data1[\"Formation\"].unique()}')\n",
    "print(f'共有{len(data1[\"WELL_encoded\"].unique())}口井,所有井名称：{data1[\"WELL_encoded\"].unique()}')\n",
    "print(f'共有{len(data1[\"LITHOLOGY\"].unique())}种岩性,所有岩性名称：{data1[\"LITHOLOGY\"].unique()}')\n",
    "print(f'共有{len(data1[\"GROUP_encoded\"].unique())}种group,所有group名称：{data1[\"GROUP_encoded\"].unique()}')\n",
    "data1.LITHOLOGY.unique()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def drop_columns(data, *args):\n",
    "\n",
    "    '''\n",
    "    function used to drop columns.\n",
    "    args::\n",
    "      data:  dataframe to be operated on\n",
    "      *args: a list of columns to be dropped from the dataframe\n",
    "\n",
    "    return: returns a dataframe with the columns dropped\n",
    "    '''\n",
    "\n",
    "    columns = []\n",
    "    for _ in args:\n",
    "        columns.append(_)\n",
    "\n",
    "    data = data.drop(columns, axis=1)\n",
    "\n",
    "    return data\n",
    "\n",
    "def process(data):\n",
    "\n",
    "    '''\n",
    "    function to process dataframe by replacing missing, infinity values with -999\n",
    "\n",
    "    args::\n",
    "      data:  dataframe to be operated on\n",
    "\n",
    "    returns dataframe with replaced values\n",
    "    '''\n",
    "\n",
    "    cols = list(data.columns)\n",
    "    for _ in cols:\n",
    "\n",
    "        data[_] = np.where(data[_] == np.inf, -999, data[_])\n",
    "        data[_] = np.where(data[_] == np.nan, -999, data[_])\n",
    "        data[_] = np.where(data[_] == -np.inf, -999, data[_])\n",
    "\n",
    "    return data\n",
    "\n",
    "def show_evaluation(pred, true):\n",
    "\n",
    "  '''\n",
    "\n",
    "  function to show model performance and evaluation\n",
    "  args:\n",
    "    pred: predicted value(a list)\n",
    "    true: actual values (a list)\n",
    "\n",
    "  prints the custom metric performance, accuracy and F1 score of predictions\n",
    "\n",
    "  '''\n",
    "\n",
    "  print(f'Default score: {score(true.values, pred)}')\n",
    "  print(f'Accuracy is: {accuracy_score(true, pred)}')\n",
    "  print(f'F1 is: {f1_score(pred, true.values, average=\"weighted\")}')\n",
    "\n",
    "\n",
    "#Paulo Bestagini's feature augmentation technique from SEG 2016 ML competition\n",
    "#Link : https://github.com/seg/2016-ml-contest/tree/master/ispl\n",
    "\n",
    "# Feature windows concatenation function\n",
    "def augment_features_window(X, N_neig):\n",
    "\n",
    "    # Parameters\n",
    "    N_row = X.shape[0]\n",
    "    N_feat = X.shape[1]\n",
    "\n",
    "    # Zero padding\n",
    "    X = np.vstack((np.zeros((N_neig, N_feat)), X, (np.zeros((N_neig, N_feat)))))\n",
    "\n",
    "    # Loop over windows\n",
    "    X_aug = np.zeros((N_row, N_feat*(2*N_neig+1)))\n",
    "    for r in np.arange(N_row)+N_neig:\n",
    "        this_row = []\n",
    "        for c in np.arange(-N_neig,N_neig+1):\n",
    "            this_row = np.hstack((this_row, X[r+c]))\n",
    "        X_aug[r-N_neig] = this_row\n",
    "\n",
    "    return X_aug\n",
    "\n",
    "# Feature gradient computation function\n",
    "def augment_features_gradient(X, depth):\n",
    "\n",
    "    # Compute features gradient\n",
    "    d_diff = np.diff(depth).reshape((-1, 1))\n",
    "    d_diff[d_diff==0] = 0.001\n",
    "    X_diff = np.diff(X, axis=0)\n",
    "    X_grad = X_diff / d_diff\n",
    "\n",
    "    # Compensate for last missing value\n",
    "    X_grad = np.concatenate((X_grad, np.zeros((1, X_grad.shape[1]))))\n",
    "\n",
    "    return X_grad\n",
    "\n",
    "# Feature augmentation function\n",
    "def augment_features(X, well, depth, N_neig=1):\n",
    "\n",
    "    # Augment features\n",
    "    X_aug = np.zeros((X.shape[0], X.shape[1]*(N_neig*2+2)))\n",
    "    for w in np.unique(well):\n",
    "        w_idx = np.where(well == w)[0]\n",
    "        X_aug_win = augment_features_window(X[w_idx, :], N_neig)\n",
    "        X_aug_grad = augment_features_gradient(X[w_idx, :], depth[w_idx])\n",
    "        X_aug[w_idx, :] = np.concatenate((X_aug_win, X_aug_grad), axis=1)\n",
    "\n",
    "    # Find padded rows\n",
    "    padded_rows = np.unique(np.where(X_aug[:, 0:7] == np.zeros((1, 7)))[0])\n",
    "\n",
    "    return X_aug, padded_rows\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "\n",
    "    '''\n",
    "    custom metric used for evaluation\n",
    "    args:\n",
    "      y_true: actual prediction\n",
    "      y_pred: predictions made\n",
    "    '''\n",
    "\n",
    "    S = 0.0\n",
    "    y_true = y_true.astype(int)\n",
    "    y_pred = y_pred.astype(int)\n",
    "    for i in range(0, y_true.shape[0]):\n",
    "        S -= A[y_true[i], y_pred[i]]\n",
    "    return S/y_true.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 所有的辅助函数\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "PWD = 'Force-2020-Machine-Learning-competition/lithology_competition/data'\n",
    "data_dir = 'data/FORCE_2020_TRAIN.csv'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "A = np.load(PWD + 'penalty_matrix.npy')\n",
    "data = pd.read_csv(data_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Model():\n",
    "\n",
    "    def __init__(self, train, test) -> None:\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "\n",
    "    def __call__(self, plot = True):\n",
    "\n",
    "      return self.fit(plot)\n",
    "\n",
    "\n",
    "\n",
    "    def preprocess(self, train, test):\n",
    "\n",
    "        '''\n",
    "        method to prepare datasets for training and predictions\n",
    "        accepts both the train and test dataframes as arguments\n",
    "\n",
    "        returns the prepared train, test datasets along with the\n",
    "        lithology labels and numbers which is needed for preparing\n",
    "        the submission file\n",
    "\n",
    "        '''\n",
    "\n",
    "        #concatenating both train and test datasets for easier and uniform processing\n",
    "\n",
    "        ntrain = train.shape[0]\n",
    "        ntest = test.shape[0]\n",
    "        target = train.FORCE_2020_LITHOFACIES_LITHOLOGY.copy()\n",
    "        df = pd.concat((train, test)).reset_index(drop=True)\n",
    "\n",
    "        #mapping the lithology labels to ordinal values for better modelling\n",
    "\n",
    "        lithology = train['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "\n",
    "        lithology_numbers = {30000: 0,\n",
    "                        65030: 1,\n",
    "                        65000: 2,\n",
    "                        80000: 3,\n",
    "                        74000: 4,\n",
    "                        70000: 5,\n",
    "                        70032: 6,\n",
    "                        88000: 7,\n",
    "                        86000: 8,\n",
    "                        99000: 9,\n",
    "                        90000: 10,\n",
    "                        93000: 11}\n",
    "\n",
    "        lithology1 = lithology.map(lithology_numbers)\n",
    "\n",
    "        #implementing Bestagini's augmentation procedure\n",
    "\n",
    "        train_well = train.WELL.values\n",
    "        train_depth = train.DEPTH_MD.values\n",
    "\n",
    "        test_well = test.WELL.values\n",
    "        test_depth = test.DEPTH_MD.values\n",
    "        '''to be continued...\n",
    "        #this was done here for ease as the datasets would undergo some transformations\n",
    "        #that would make it uneasy to perform the augmentation technique'''\n",
    "\n",
    "\n",
    "\n",
    "        print(f'shape of concatenated dataframe before dropping columns {df.shape}')\n",
    "\n",
    "        cols = ['FORCE_2020_LITHOFACIES_CONFIDENCE', 'SGR', 'DTS', 'RXO', 'ROPA'] #columns to be dropped\n",
    "        df = drop_columns(df, *cols)\n",
    "        print(f'shape of dataframe after dropping columns {df.shape}')\n",
    "        print(f'{cols} were dropped')\n",
    "\n",
    "        #Label encoding the GROUP, FORMATION and WELLS features as these improved the performance of the models on validations\n",
    "\n",
    "        df['GROUP_encoded'] = df['GROUP'].astype('category')\n",
    "        df['GROUP_encoded'] = df['GROUP_encoded'].cat.codes\n",
    "        df['FORMATION_encoded'] = df['FORMATION'].astype('category')\n",
    "        df['FORMATION_encoded'] = df['FORMATION_encoded'].cat.codes\n",
    "        df['WELL_encoded'] = df['WELL'].astype('category')\n",
    "        df['WELL_encoded'] = df['WELL_encoded'].cat.codes\n",
    "        print(f'shape of dataframe after label encoding columns {df.shape}')\n",
    "\n",
    "\n",
    "        #FURTHER PREPRATION TO SPLIT DATAFRAME INTO TRAIN AND TEST DATASETS AFTER PREPRATION\n",
    "        print(f'Splitting concatenated dataframe into training and test datasets...')\n",
    "        df = df.drop(['WELL', 'GROUP', 'FORMATION'], axis=1)\n",
    "        print(df.shape)\n",
    "\n",
    "        df = df.fillna(-999)\n",
    "        df = process(df)\n",
    "        data = df.copy()\n",
    "\n",
    "        train2 = data[:ntrain].copy()\n",
    "        train2.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)\n",
    "\n",
    "        test2 = data[ntrain:(ntest+ntrain)].copy()\n",
    "        test2.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)\n",
    "        test2 = test2.reset_index(drop=True)\n",
    "\n",
    "        traindata = train2\n",
    "        testdata = test2\n",
    "\n",
    "        print(f'Shape of train and test datasets before augmentation {traindata.shape, testdata.shape}')\n",
    "\n",
    "        traindata1, padded_rows = augment_features(pd.DataFrame(traindata).values, train_well, train_depth)\n",
    "        testdata1, padded_rows = augment_features(pd.DataFrame(testdata).values, test_well, test_depth)\n",
    "\n",
    "        print(f'Shape of train and test datasets after augmentation {traindata1.shape, testdata1.shape}')\n",
    "\n",
    "        return traindata1, testdata1, lithology1, lithology_numbers\n",
    "    def fit(self, plot):\n",
    "\n",
    "      '''\n",
    "      method to train model and make predictions\n",
    "\n",
    "      returns the test predictions, trained model, and lithology numbers used for making the submission file\n",
    "      '''\n",
    "\n",
    "      traindata1, testdata1, lithology1, lithology_numbers = self.preprocess(self.train, self.test)\n",
    "\n",
    "      #using a 10-fold stratified cross-validation technique and seting the shuffle parameter to true\n",
    "      #as this improved the validation performance better\n",
    "\n",
    "      split = 10\n",
    "      kf = StratifiedKFold(n_splits=split, shuffle=True)\n",
    "\n",
    "      open_test = np.zeros((len(testdata1), 12))\n",
    "\n",
    "      models = []\n",
    "\n",
    "      model_1dcnn = self.get_model_1dcnn()\n",
    "      model_lstm = self.get_model_lstm()\n",
    "      model_bilstm = self.get_model_bilstm()\n",
    "      model_tcn = self.get_model_tcn()\n",
    "      models.append(model_1dcnn, model_lstm, model_bilstm, model_tcn)\n",
    "\n",
    "      # TODO: 添加 4 个 模型\n",
    "      model = models[0]\n",
    "      # model.summary()\n",
    "      model.fit(traindata1, lithology1)\n",
    "\n",
    "\n",
    "      i = 1\n",
    "      for (train_index, test_index) in kf.split(pd.DataFrame(traindata1), pd.DataFrame(lithology1)):\n",
    "        X_train, X_test = pd.DataFrame(traindata1).iloc[train_index], pd.DataFrame(traindata1).iloc[test_index]\n",
    "        Y_train, Y_test = pd.DataFrame(lithology1).iloc[train_index],pd.DataFrame(lithology1).iloc[test_index]\n",
    "\n",
    "        model.fit(X_train, Y_train, early_stopping_rounds=100, eval_set=[(X_test, Y_test)], verbose=100)\n",
    "        prediction = model.predict(X_test)\n",
    "        print(show_evaluation(prediction, Y_test))\n",
    "\n",
    "        print(f'-----------------------FOLD {i}---------------------')\n",
    "        i+=1\n",
    "\n",
    "        open_test += model.predict_proba(pd.DataFrame(testdata1))\n",
    "\n",
    "      open_test= pd.DataFrame(open_test/split)\n",
    "\n",
    "      open_test = np.array(pd.DataFrame(open_test).idxmax(axis=1))\n",
    "\n",
    "      print('---------------CROSS VALIDATION COMPLETE')\n",
    "      print('----------------TEST EVALUATION------------------')\n",
    "\n",
    "\n",
    "      if plot: self.plot_feat_imp(model)\n",
    "      return open_test, model, lithology_numbers\n",
    "\n",
    "\n",
    "    def plot_feat_imp(self, model):\n",
    "        feat_imp = pd.Series(model.feature_importances_).sort_values(ascending=False)\n",
    "        plt.figure(figsize=(12,8))\n",
    "        feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')\n",
    "\n",
    "    def make_submission_file(self, filename):\n",
    "\n",
    "      '''\n",
    "      method to train model, make prediction and create submission file\n",
    "      args::\n",
    "        filename: name to save submission file as (string)\n",
    "      '''\n",
    "\n",
    "      self.filename = filename\n",
    "\n",
    "      prediction, model, lithology_numbers = self.fit(plot=False)\n",
    "\n",
    "      path = PWD\n",
    "\n",
    "      test = pd.read_csv(PWD + 'Test.csv', sep=';')\n",
    "\n",
    "      category_to_lithology = {y:x for x,y in lithology_numbers.items()}\n",
    "      test_prediction_for_submission = np.vectorize(category_to_lithology.get)(prediction)\n",
    "      np.savetxt(path+filename+'.csv', test_prediction_for_submission, header='lithology', fmt='%i')\n",
    "\n",
    "    def get_model_lstm(self):\n",
    "        pass\n",
    "\n",
    "    def get_model_bilstm(self):\n",
    "        pass\n",
    "\n",
    "    def get_model_tcn(self):\n",
    "        pass\n",
    "\n",
    "    def get_model_1dcnn(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}